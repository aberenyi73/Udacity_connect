{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM -  Support Vector Machines\n",
    "Udacity Connect Dec 2017, Antal Berenyi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "SVM is a type of classification algorithm. The goal of SVM is to take an input point and assign it to one of K number of classes.\n",
    "\n",
    "It was developed by Vladimir Vapnik and co-workers in 1995. [[Sewell](http://www.svms.org/introduction.html)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard Margin\n",
    "If we strictly enforce separation of data, it is called _hard margin classification_. The problem with that is that it only works if \n",
    "1. there are no outliers and \n",
    "1. the data is linearly separable. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"optimal-hyperplane.png\" title=\"Soft Margin Classification\" alt=\"Soft Marging Classification\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Margin Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"SoftMarginClassification.png\" width=\"80%\" title=\"Hard Margin Classification\" alt=\"Hard Marging Classification\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Soft margin classification_ comes to the rescue. It aims to balance between margin violations and keeping a wide street between classes.\n",
    "To balance _margin violations_ and keeping the separation as wide as possible, SVM algorithm has the hyper-parameter _C_ to control this balance. Reducing _C_ will generalize the model better because it emphasizes the regularization term. Large value of C gives higher weight to the individual feaures while therefore it increases variance. [[slideplayer.com](http://slideplayer.com/slide/3362142/)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Margins\n",
    "SVM is a large-margin classifier. It aims to separate classes by a hyperplane with as large a margin as possible. Fisher's idea is to reduce variance within class by applying large separation, reducing overlap. What is interesting about it is that it only considers data points that are closest to the separating hyperplane. Those points are called the support vectors. Otherwise it works like an iterative logistic least squares error classifier. Solving for the support vectors is a quadratic programming problem (http://slideplayer.com/slide/8788392/)]\n",
    "\n",
    "\n",
    "## Loss Function\n",
    "<br>Logistic regression uses log-loss function to penalize larger errors. To achieve large margins and ignore further away points SVM uses a Hinge Loss penalty function that where the loss is set to zero if a point is further than a constant distance from the line. [[slideplayer.com]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"hinge-loss.png\" width=\"70%\" alt=\"Alt text that describes the graphic\" title=\"Loss Function\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM Classification\n",
    "\n",
    "### Sensitivity to feature scales\n",
    "SVM is sensitive to feature scales. You have to normalize dimensions to be on the same scale otherwise it will give wrong answers.  The figure below right appears easier to separate linearly than the one on the left. [[Wine dataset from UCI](http://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"sphx_glr_plot_scaling_importance_001.png\" title=\"Feature Scaling\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data\n",
    "SVM needs all data points from all samples to be present to calculate support vectors. Missing data has to be imputed, which means to be filled in. There are different strategies for filling data in, for example mean within a group, scaling all vectors to the same length, etc. [[sklearn.preprocessing](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)] has many strategies to impute data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification\n",
    "Suppose we have K classes we would like to predict. Then we can combine two-class discriminant functions to act like a multi-class discriminant.\n",
    "\n",
    "### One vs. Rest\n",
    "Separate one class from all the other vectors not in class. Consider one class at at time and label all other classes as the \"other\" class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"one-vs-all.jpg\" title=\"one vs. all\" alt=\"One vs. all\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One vs. One\n",
    "In this scenario one class is comapred against each of the K-1 other classes K times, making \n",
    "\n",
    "> \n",
    "<big><big>\n",
    "$ \\begin{equation*}\n",
    "\\frac{K*(K-1)}{2}\n",
    "\\end{equation*}\n",
    "$\n",
    "</big></big>\n",
    "\n",
    "comparisons. This may be very time consuming to compute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear SVM Classification\n",
    "### Polynomial Features\n",
    "Linear SVMs are efficient and work on well on linearly separable data. Many data sets are far from being linearly separable. One approach to address this issue is to add non-linear features. Adding polynomial features may turn the data set into a linearly separable data set. In Scikit-learn you can use PolynomialFeatures transformer to add polynomial features. The Pipeline class can help to put it all together. The draback is that a huge number of features are created on the order of O(choose(n,k)), n being the order of the polynomial, k the number of features.\n",
    "\n",
    "\n",
    "### Polynomial kernel\n",
    "A polynomial kernel is like a magic trick that achieves the same goal as polynomial features without exponentiallly increasing feature count. It transforms each data point by applying the same _kernel function_ to each. This transformation in effect separates the data points into higher dimensions and data that is not separable in n dimension may become separable in n + k dimensions. [[hackerearth.com](http://blog.hackerearth.com/simple-tutorial-svm-parameter-tuning-python-r)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"kernel-trick.png\" title=\"Kernel Trick\" alt=\"Kernel Trick\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros\n",
    "* SVM is very versatile and powerful\n",
    "* Convex optimization guaratees optimality. The solution is guaranteed to be a global minimum and not a local minimum.\n",
    "* Suitable for both linearly and non-liearly separable data. \n",
    "* You only need to train C parameter and provide a kernel.\n",
    "* Works well on small as well as large dimensional data sets. The complexity of the training data is characterized by the support vector, not the entire data set.\n",
    "* It can work well with small training data set.\n",
    "\n",
    "\n",
    "## Cons\n",
    "* Not suitable for larger data sets becuase the training time can be high. O(n$^3$)\n",
    "* Less effective on noisier data sets and overlapping data points.\n",
    "\n",
    "[[hackerearth.com](http://blog.hackerearth.com/simple-tutorial-svm-parameter-tuning-python-r)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Life Application\n",
    "* Face detection\n",
    "* Bioinformatics\n",
    "* Handwriting recognition\n",
    "* Text and hypertext categorization\n",
    "* Controlling chaotic systems\n",
    "\n",
    "[[data-flair](https://data-flair.training/blogs/applications-of-svm/)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
